{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize \n",
    "import pandas as pd \n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cálculo de lectura con Pandas = 0.866044282913208\n"
     ]
    }
   ],
   "source": [
    "#exportación y lectura con pandas\n",
    "inicio = time.time()\n",
    "pd_Pandas = pd.read_csv('../RAPIDS/df_data.csv', names='uno')\n",
    "fin = time.time()\n",
    "\n",
    "calcultedPD = fin-inicio\n",
    "print(\"Cálculo de lectura con Pandas = {}\".format(calcultedPD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cálculo de transformación RAPIDS: 2.941384792327881\n"
     ]
    }
   ],
   "source": [
    "#transformacion DATA PANDAS\n",
    "\n",
    "#convertir en lista y despues en json\n",
    "startSecuenciaCPU = time.time()\n",
    "data = list(pd_Pandas['u'])\n",
    "da = []\n",
    "for i in range(0,len(data)):\n",
    "    d = data[i].split('.gz:')\n",
    "    da.append(json.loads((d[1])))\n",
    "    \n",
    "#DataFrame PANDAS\n",
    "new_data = pd.DataFrame(da)\n",
    "\n",
    "#only consservate the dat of the column 'username'\n",
    "#qhigehr to 0\n",
    "snNull = new_data[new_data.username.map(len) > 0]\n",
    "\n",
    "#eliminar username dicc y opencampus\n",
    "snDicc = snNull[snNull.username != 'dicc']\n",
    "snCampus = snDicc[snDicc.username != 'opencampus']\n",
    "\n",
    "#eliminar filas con los datos perdidos\n",
    "snCampus.dropna(axis=0, how='any')\n",
    "\n",
    "#Eliminar columnas inncesarias no tiene relevancia\n",
    "snCampus.drop(['accept_language', 'name','agent', 'page',\n",
    "               'session','event', 'event_type'], \n",
    "              axis=1, inplace=True)\n",
    "\n",
    "#Valores server = 1, browser = 2\n",
    "snCampus['event_source'].replace(['server','browser'],[1,2],inplace=True)\n",
    "\n",
    "#Valor opencampus.utpl.edu.ec = 1\n",
    "snCampus['host'].replace(['opencampus.utpl.edu.ec','soer.utpl.edu.ec'],\n",
    "                         [1,2],inplace=True)\n",
    "\n",
    "\n",
    "#eliminar filas con datos perdidos\n",
    "snCampus.dropna(axis=0, how='any')\n",
    "\n",
    "#soltar duplicados\n",
    "snCampus.sort_values(\"username\") \n",
    "snCampus.drop_duplicates(subset='username',\n",
    "                         keep=False, inplace=False)\n",
    "\n",
    "dataLimpia = snCampus\n",
    "\n",
    "#Dividir columna context\n",
    "contextData = dataLimpia['context'].values.tolist()\n",
    "new_contextData = pd.DataFrame(contextData, columns= ['course_user_tags',\n",
    "                                                      'course_id', 'path',\n",
    "                                                      'org_id', 'user_id'])\n",
    "generalData = pd.merge(dataLimpia.reset_index(),\n",
    "                       new_contextData.reset_index(),\n",
    "                       left_index=True, right_index=True)\n",
    "generalData = generalData.drop(['context','index_y','course_user_tags',\n",
    "                                'course_id','path'], axis=1)\n",
    "\n",
    "#eliminar filas con datos perdidos\n",
    "generalData.dropna(axis=0, how='any')\n",
    "\n",
    "#dividir columna time\n",
    "df_picru = pd.DataFrame(generalData['time'])\n",
    "datenew = df_picru.apply(lambda x: pd.to_datetime(x,errors = 'coerce', format = '%Y-%m-%d'))\n",
    "\n",
    "datenew['Day'] = datenew['time'].dt.day\n",
    "datenew['Month'] = datenew['time'].dt.month\n",
    "datenew['Year'] = datenew['time'].dt.year\n",
    "datenew['Hour'] = datenew['time'].dt.time\n",
    "\n",
    "datenew = datenew.drop(['time','Hour'], axis=1)\n",
    "\n",
    "generalData = pd.merge(generalData.reset_index(),\n",
    "                       datenew.reset_index(),\n",
    "                       left_index=True, right_index=True)\n",
    "\n",
    "generalData = generalData.drop(['time','index_x','index_y'], axis=1)\n",
    "\n",
    "#eliminar filas con datos perdidos\n",
    "generalData.dropna(axis=0, how='any')\n",
    "\n",
    "endSecuencuaCPU = time.time()\n",
    "cpuPandas = endSecuencuaCPU - startSecuenciaCPU\n",
    "\n",
    "print('Cálculo de transformación RAPIDS:', cpuPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information with RAPIDS of all Dataset\n",
    "print('Names columns:',generalData.columns)\n",
    "print('Dimension DataFrame:',generalData.shape)\n",
    "print('Verificar valores nulos por cada columna:', generalData.isnull().sum())\n",
    "\n",
    "print('Information DATA general again with PANDAS:')\n",
    "generalData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar la data entendible en un nuevo csv\n",
    "start = time.time()\n",
    "da_data = 'data_mejora.csv'\n",
    "generalData.to_csv(da_data)\n",
    "end = time.time()\n",
    "\n",
    "calculated = end - start\n",
    "\n",
    "print(\"Cálculo de lectura con DASK_CUDF = {}\".format(calculated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_data = pd.read_csv('data_mejora.csv')\n",
    "\n",
    "#Preprocesamiento con Pandas\n",
    "nueva_data.isnull().sum()\n",
    "valor = 'UTPL'\n",
    "nueva_data['org_id'] = nueva_data['org_id'].fillna(valor)\n",
    "nueva_data.org_id = nueva_data.org_id.map({'UTPL':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA Splitting con Pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Divide datasets in train and test\n",
    "#datatrain 80% and datatest 20%\n",
    "data_train, data_test = train_test_split(nueva_data, test_size=0.2)\n",
    "\n",
    "print(f'Original dataset: {nueva_data.shape[0]} elemnts')\n",
    "print(f'Data train: {nueva_data.shape[0]} elements')\n",
    "print(f'Data test: {nueva_data.shape[0]} elements')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
